{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter6.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeh8911/Deep_Learning_from_Scratch_Study/blob/master/Chapter6_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMAgbxpG1eLZ",
        "colab_type": "text"
      },
      "source": [
        "# chapter6. Technique about learning\n",
        "1. backpropagation method\n",
        "2. weight initial value\n",
        "3. batch normalization\n",
        "4. restrain overfitting\n",
        "5. hyper-parameter optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_zZYJWkjlkJ",
        "colab_type": "code",
        "outputId": "e93fc977-f23c-4bd0-c6d6-652dd5393465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/gdrive/My Drive/github/deep-learning-from-scratch/ch05\")\n",
        "sys.path.append(os.pardir)\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from collections import OrderedDict \n",
        "from IPython.display import clear_output\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"env setting finished!\")\n",
        "\n"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "env setting finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBxqsUCO23XM",
        "colab_type": "code",
        "outputId": "e3a1ae1d-a9b6-4ca6-f007-44236ceaa6d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    load_mnist(flatten = True, normalize = True, one_hot_label = True)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(t_train.shape)\n",
        "print(x_test.shape)\n",
        "print(t_test.shape)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sPoXLn7lop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    \n",
        "    \n",
        "def softmax(a):\n",
        "    c = np.max(a)\n",
        "    exp_a = np.exp(a - c)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "\n",
        "    return y\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "    delta = 1e-7\n",
        "    \n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(t * np.log(y + delta)) / batch_size\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = 1/(1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out)*self.out\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "    \n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis = 0)\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        \n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        \n",
        "        return self.loss\n",
        "    \n",
        "    def backward(self, dout = 1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2f1AvdD4GjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# costomized network\n",
        "class cosNet:\n",
        "    def __init__(self, input_size = 1, hidden_size = 1, output_size = 1, weight_init_std = 0.01):\n",
        "        \n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "        \n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = \\\n",
        "                Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['ReLU1'] = ReLU()\n",
        "        self.layers['Affine2'] = \\\n",
        "                Affine(self.params['W2'], self.params['b2']);\n",
        "        \n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis = 1)\n",
        "        \n",
        "        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        \n",
        "        return accuracy\n",
        "    \n",
        "    def gradient(self, x, t):\n",
        "        self.loss(x, t)\n",
        "        \n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "            \n",
        "        grads = {}\n",
        "        grads['W1'] = self.layers['Affine1'].dW\n",
        "        grads['b1'] = self.layers['Affine1'].db\n",
        "        grads['W2'] = self.layers['Affine2'].dW\n",
        "        grads['b2'] = self.layers['Affine2'].db\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def display(self):\n",
        "        clear_output()\n",
        "        plt.figure(1)\n",
        "        plt.title(\"Network\")\n",
        "        plt.show()\n",
        "        return False\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkqF4u7tjpkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# backpropagation methods\n",
        "class SGD:\n",
        "    def __init__(self, lr = 0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]\n",
        "            \n",
        "            \n",
        "class Momentum:\n",
        "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "                \n",
        "            for key in params.keys():\n",
        "                self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n",
        "                params[key] += self.v\n",
        "                \n",
        "class AdaGrad:\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            \n",
        "        for key, val in params.item():\n",
        "            self.h[key] = np.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            slef.h[key] += grads[key]*grads[key]\n",
        "            params[key] -= self.lr*grads[key]/np.sqrt(self.h[key] + 1e-7)\n",
        "            \n",
        "            \n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj-0gCqkDIDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "414bd45e-dcd3-4eea-cfb6-a50ad3041e8a"
      },
      "source": [
        "net = cosNet(input_size = x_train.shape[1], hidden_size = 50, output_size = 10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    grad = net.gradient(x_batch, t_batch)\n",
        "    \n",
        "    for key in ('W1','b1','W2','b2'):\n",
        "        net.params[key] -= learning_rate * grad[key]\n",
        "        \n",
        "    loss = net.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = net.accuracy(x_train, t_train)\n",
        "        test_acc = net.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)\n",
        "\n",
        "        \n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "784 50 10 0.01\n",
            "0.12131666666666667 0.1303\n",
            "0.10218333333333333 0.101\n",
            "0.09871666666666666 0.098\n",
            "0.09871666666666666 0.098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: invalid value encountered in subtract\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:85: RuntimeWarning: invalid value encountered in less_equal\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.09871666666666666 0.098\n",
            "0.09871666666666666 0.098\n",
            "0.09871666666666666 0.098\n",
            "0.09871666666666666 0.098\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-eacd55e657a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'b2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-d69338898a34>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-90-447812327a59>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqrjm_s0RA7N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a26ae1ce-0593-4d75-d318-b950082f821e"
      },
      "source": [
        "\n",
        "plt.figure(2)\n",
        "plt.plot(train_acc_list)\n",
        "plt.plot(test_acc_list)\n",
        "plt.ylim([-0.1, 1.1])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.1, 1.1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAD/1JREFUeJzt3X+s3XV9x/Hnqy3lNwL2Ull7Szsp\nzo7pIHeMzWzTgaYw05rsRyBzw43YZBHHNuKCc2ELSxZ/LLotYzqiDOccjDFlzVaHRnEmizAuKmjB\n4rUobQW5AuIPFKh9749zSi7Xtvfc9tx+7/34fCRNz/d7Pj3fd9vb5/2e77mnN1WFJKkti7oeQJI0\nfMZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQUu6OvCyZctq9erVXR1ekhaku+66\n6xtVNTLTus7ivnr1asbHx7s6vCQtSEm+Osg6L8tIUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhL\nUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoNmjHuS65I8kuQL+7k/Sf42yUSSe5KcPfwxJUmz\nMciZ+/XA+gPcfwGwtv9jE/DuQx9LknQoZox7VX0KeOwASzYC/1Q9twMnJjl1WANKkmZvGNfcVwA7\npmzv7O/7IUk2JRlPMj45OTmEQ0uS9uWwvqBaVddW1VhVjY2MzPhdoiRJB2kYcd8FjE7ZXtnfJ0nq\nyDDivhn47f5XzZwLPFFVDw3hcSVJB2nGb5Cd5Abg5cCyJDuBPwOOAKiq9wBbgAuBCeBJ4HfmalhJ\n0mBmjHtVXTzD/QW8YWgTSZIOme9QlaQGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBx\nl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QG\nGXdJapBxl6QGGXdJapBxl6QGGXdJapBxl6QGDRT3JOuTbEsykeTKfdy/KsltST6b5J4kFw5/VEnS\noGaMe5LFwDXABcA64OIk66Yt+1Pgpqo6C7gI+PthDypJGtwgZ+7nABNVtb2qngZuBDZOW1PACf3b\nzwO+NrwRJUmztWSANSuAHVO2dwI/O23NnwMfTfJG4Fjg/KFMJ0k6KMN6QfVi4PqqWglcCHwgyQ89\ndpJNScaTjE9OTg7p0JKk6QaJ+y5gdMr2yv6+qS4FbgKoqk8DRwHLpj9QVV1bVWNVNTYyMnJwE0uS\nZjRI3O8E1iZZk2QpvRdMN09b8yBwHkCSF9OLu6fmktSRGeNeVbuBy4BbgfvofVXM1iRXJ9nQX3YF\n8PokdwM3AK+rqpqroSVJBzbIC6pU1RZgy7R9V025fS/wsuGOJkk6WL5DVZIaZNwlqUHGXZIaZNwl\nqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHG\nXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaNFDck6xPsi3J\nRJIr97PmN5Lcm2Rrkn8Z7piSpNlYMtOCJIuBa4BXAjuBO5Nsrqp7p6xZC7wZeFlVPZ7klLkaWJI0\ns0HO3M8BJqpqe1U9DdwIbJy25vXANVX1OEBVPTLcMSVJszFI3FcAO6Zs7+zvm+oM4Iwk/5vk9iTr\nhzWgJGn2ZrwsM4vHWQu8HFgJfCrJT1XVN6cuSrIJ2ASwatWqIR1akjTdIGfuu4DRKdsr+/um2gls\nrqpnquoB4H56sX+Oqrq2qsaqamxkZORgZ5YkzWCQuN8JrE2yJslS4CJg87Q1t9A7ayfJMnqXabYP\ncU5J0izMGPeq2g1cBtwK3AfcVFVbk1ydZEN/2a3Ao0nuBW4D3lRVj87V0JKkA0tVdXLgsbGxGh8f\n7+TYkrRQJbmrqsZmWuc7VCWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZd\nkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkhpk\n3CWpQcZdkhpk3CWpQcZdkhpk3CWpQcZdkho0UNyTrE+yLclEkisPsO5Xk1SSseGNKEmarRnjnmQx\ncA1wAbAOuDjJun2sOx64HLhj2ENKkmZnkDP3c4CJqtpeVU8DNwIb97HuL4C3Ad8f4nySpIMwSNxX\nADumbO/s73tWkrOB0ar6rwM9UJJNScaTjE9OTs56WEnSYA75BdUki4B3AlfMtLaqrq2qsaoaGxkZ\nOdRDS5L2Y5C47wJGp2yv7O/b63jgTOCTSb4CnAts9kVVSerOIHG/E1ibZE2SpcBFwOa9d1bVE1W1\nrKpWV9Vq4HZgQ1WNz8nEkqQZzRj3qtoNXAbcCtwH3FRVW5NcnWTDXA8oSZq9JYMsqqotwJZp+67a\nz9qXH/pYkqRD4TtUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTc\nJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalB\nxl2SGmTcJalBxl2SGmTcJalBA8U9yfok25JMJLlyH/f/UZJ7k9yT5ONJThv+qJKkQc0Y9ySLgWuA\nC4B1wMVJ1k1b9llgrKpeAtwMvH3Yg0qSBjfImfs5wERVba+qp4EbgY1TF1TVbVX1ZH/zdmDlcMeU\nJM3GIHFfAeyYsr2zv29/LgU+sq87kmxKMp5kfHJycvApJUmzMtQXVJO8FhgD3rGv+6vq2qoaq6qx\nkZGRYR5akjTFkgHW7AJGp2yv7O97jiTnA28BfqmqnhrOeJKkgzHImfudwNoka5IsBS4CNk9dkOQs\n4B+ADVX1yPDHlCTNxoxxr6rdwGXArcB9wE1VtTXJ1Uk29Je9AzgO+Lckn0uyeT8PJ0k6DAa5LENV\nbQG2TNt31ZTb5w95LknSIfAdqpLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLUIOMuSQ0y7pLU\noIHeoTqffOX2W9j9+Q+zaNnpHHPqizhp9MUcecrpcMTRXY8mSfPGgov79okvcubOT3LKrlvg7t6+\nPYRHF43w2NGjfO/4NfD80zlq+VpOWrWOZSvXsnjJEd0OLUmHWaqqkwOPjY3V+Pj4rH/dnj3F5Hee\nYtfDX+ebu7bx1MP3s+ixL3PMtx/g5Kd2MrpnFyfkyWfXP1OLeWjRC/jGkaN89/jT2HPSCzly+Rmc\nsPLFnLpiDSceu5Qkw/ytSdKcSXJXVY3NtG7BnbkvWhSWn3AUy084Dc44DXjVc+5/+pkf8ODDu3j8\nwXt58uFt1KNf5uhvPcDJ33uQdZN3cdTkM3B/b+2TdSTbeT57fOlB0mH0yFmX87LXbJrTYyy4uM9k\n6RGLWTW6ilWjq4D1z71zzx6+M/lVHn3wPr770Bf5weQES77zEKGbZy+SfjSNnLJ8zo/RXNwPaNEi\njlu+huOWrwEu7HoaSZozXo+QpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZ\nd0lqkHGXpAYZd0lq0EBxT7I+ybYkE0mu3Mf9Ryb51/79dyRZPexBJUmDmzHuSRYD1wAXAOuAi5Os\nm7bsUuDxqjodeBfwtmEPKkka3CBn7ucAE1W1vaqeBm4ENk5bsxF4f//2zcB58dsbSVJnBon7CmDH\nlO2d/X37XFNVu4EngOcPY0BJ0uwd1hdUk2xKMp5kfHJy8nAeWpJ+pAwS913A6JTtlf19+1yTZAnw\nPODR6Q9UVddW1VhVjY2MjBzcxJKkGQ0S9zuBtUnWJFkKXARsnrZmM3BJ//avAZ+oKr8xqSR1ZMbv\noVpVu5NcBtwKLAauq6qtSa4GxqtqM/A+4ANJJoDH6H0CkCR1ZKBvkF1VW4At0/ZdNeX294FfH+5o\nkqSD5TtUJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2S\nGmTcJalB6eq/XU8yCXz1IH/5MuAbQxxnWJxrdpxr9ubrbM41O4cy12lVNeN3O+os7ociyXhVjXU9\nx3TONTvONXvzdTbnmp3DMZeXZSSpQcZdkhq0UON+bdcD7IdzzY5zzd58nc25ZmfO51qQ19wlSQe2\nUM/cJUkHsODinmR9km1JJpJc2fU8AElGk9yW5N4kW5Nc3vVMUyVZnOSzSf6z61n2SnJikpuTfDHJ\nfUl+ruuZAJL8Yf/v8AtJbkhyVEdzXJfkkSRfmLLv5CQfS/Kl/s8nzZO53tH/e7wnyYeTnDgf5ppy\n3xVJKsmy+TJXkjf2/8y2Jnn7XBx7QcU9yWLgGuACYB1wcZJ13U4FwG7giqpaB5wLvGGezLXX5cB9\nXQ8xzd8A/11VPwG8lHkwX5IVwO8DY1V1JrAYuKijca4H1k/bdyXw8apaC3y8v324Xc8Pz/Ux4Myq\neglwP/Dmwz0U+56LJKPAq4AHD/dAfdczba4krwA2Ai+tqp8E/mouDryg4g6cA0xU1faqehq4kd4f\nUqeq6qGq+kz/9rfphWpFt1P1JFkJ/Arw3q5n2SvJ84BfBN4HUFVPV9U3u53qWUuAo5MsAY4BvtbF\nEFX1KeCxabs3Au/v334/8JrDOhT7nquqPlpVu/ubtwMr58Ncfe8C/hjo5MXF/cz1e8Bbq+qp/ppH\n5uLYCy3uK4AdU7Z3Mk8iuleS1cBZwB3dTvKsv6b3wb2n60GmWANMAv/Yv1z03iTHdj1UVe2idxb1\nIPAQ8ERVfbTbqZ5jeVU91L/9MLC8y2H243eBj3Q9BECSjcCuqrq761mmOQP4hSR3JPmfJD8zFwdZ\naHGf15IcB/w78AdV9a15MM+rgUeq6q6uZ5lmCXA28O6qOgv4Lt1cYniO/jXsjfQ++fwYcGyS13Y7\n1b5V78vc5tWXuiV5C71LlB+cB7McA/wJcFXXs+zDEuBkepdw3wTclCTDPshCi/suYHTK9sr+vs4l\nOYJe2D9YVR/qep6+lwEbknyF3iWsX07yz92OBPSece2sqr3Pbm6mF/uunQ88UFWTVfUM8CHg5zue\naaqvJzkVoP/znDydPxhJXge8GvjNmh9fX/1Cep+k7+5//K8EPpPkBZ1O1bMT+FD1/B+9Z9VDf7F3\nocX9TmBtkjVJltJ7sWtzxzPR/6z7PuC+qnpn1/PsVVVvrqqVVbWa3p/VJ6qq8zPRqnoY2JHkRf1d\n5wH3djjSXg8C5yY5pv93eh7z4IXeKTYDl/RvXwL8R4ezPCvJenqX/jZU1ZNdzwNQVZ+vqlOqanX/\n438ncHb/Y69rtwCvAEhyBrCUOfjPzRZU3Psv2lwG3ErvH91NVbW126mA3hnyb9E7M/5c/8eFXQ81\nz70R+GCSe4CfBv6y43noP5O4GfgM8Hl6/z46eYdjkhuATwMvSrIzyaXAW4FXJvkSvWcZb50nc/0d\ncDzwsf7H/nvmyVyd289c1wE/3v/yyBuBS+bi2Y7vUJWkBi2oM3dJ0mCMuyQ1yLhLUoOMuyQ1yLhL\nUoOMuyQ1yLhLUoOMuyQ16P8Bx8cpqxjrpAgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}