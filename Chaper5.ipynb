{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chaper5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeh8911/Deep_Learning_from_Scratch_Study/blob/master/Chaper5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az5XiS_XrBJH",
        "colab_type": "code",
        "outputId": "6a09bea4-e601-4a16-ea5f-2172f32d1a3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys, os\n",
        "os.chdir(\"/content/gdrive/My Drive/github/deep-learning-from-scratch/ch05\")\n",
        "sys.path.append(os.pardir)\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "print(\"env setting finished!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "env setting finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m55sdUVjsh2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation simple layer\n",
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        \n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y\n",
        "        dy = dout * self.x\n",
        "        \n",
        "        return dx, dy\n",
        "    \n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(slef, x, y):\n",
        "        out = x + y\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "        return dx, dy\n",
        "    \n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx\n",
        "    \n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis = 0)\n",
        "        \n",
        "        return dx\n",
        "    \n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "        \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "    \n",
        "    def backward(self, dout = 1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size\n",
        "        \n",
        "        return dx\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGvovlVGs7AC",
        "colab_type": "code",
        "outputId": "95d0c4aa-d668-4812-c9bd-88f90d5987d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# test simple layer\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# layers\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# forward\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "print(price)\n",
        "\n",
        "# backward\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple_num, dapple)\n",
        "print(dorange_num, dorange)\n",
        "print(dtax)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2\n",
            "165.0 3.3000000000000003\n",
            "650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rEjm8CGzuVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size, \n",
        "                weight_init_std = 0.01):\n",
        "        \n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "        \n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        \n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis = 1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis = 1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "    \n",
        "    \n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def gradient(self, x, t):\n",
        "        self.loss(x, t)\n",
        "        \n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "            \n",
        "        grads = {}\n",
        "        grads['W1'] = self.layers['Affine1'].dW\n",
        "        grads['b1'] = self.layers['Affine1'].db\n",
        "        grads['W2'] = self.layers['Affine2'].dW\n",
        "        grads['b2'] = self.layers['Affine2'].db\n",
        "        \n",
        "        return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp8vAD003GMW",
        "colab_type": "code",
        "outputId": "245bcec1-21e4-4aeb-f9e6-4f114a0cc904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# Validate back propagation with numerical gradient\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]))\n",
        "    \n",
        "    print(key + \" : \" + str(diff))\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 : 4.671881703675071e-10\n",
            "b1 : 2.9015709926633396e-09\n",
            "W2 : 5.537309846993289e-09\n",
            "b2 : 1.397943305336824e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPgaUbvx7q53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "58cbbbc1-94bb-4fb7-dd77-35b679e02666"
      },
      "source": [
        "# Implementation neural network using back propagation\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "load_mnist(normalize = True, one_hot_label = True)\n",
        "\n",
        "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size/batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    for key in ('W1','b1','W2','b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "        \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0.:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        \n",
        "        print(\"epoch : \", i/iter_per_epoch, \" acc : \", train_acc, test_acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch :  0.0  acc :  0.12915 0.132\n",
            "epoch :  1.0  acc :  0.9025833333333333 0.9036\n",
            "epoch :  2.0  acc :  0.9233666666666667 0.9243\n",
            "epoch :  3.0  acc :  0.9356666666666666 0.9356\n",
            "epoch :  4.0  acc :  0.94495 0.9439\n",
            "epoch :  5.0  acc :  0.9516 0.9521\n",
            "epoch :  6.0  acc :  0.9561333333333333 0.9556\n",
            "epoch :  7.0  acc :  0.9603666666666667 0.9583\n",
            "epoch :  8.0  acc :  0.9642 0.96\n",
            "epoch :  9.0  acc :  0.9678 0.9632\n",
            "epoch :  10.0  acc :  0.97075 0.9638\n",
            "epoch :  11.0  acc :  0.9723333333333334 0.9653\n",
            "epoch :  12.0  acc :  0.9732333333333333 0.9667\n",
            "epoch :  13.0  acc :  0.9767166666666667 0.9681\n",
            "epoch :  14.0  acc :  0.97785 0.97\n",
            "epoch :  15.0  acc :  0.97855 0.9695\n",
            "epoch :  16.0  acc :  0.9801333333333333 0.9714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5x_Zmgb9lcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "6f562ef4-cc12-48db-95fe-d093461cb5a3"
      },
      "source": [
        "plt.plot(train_acc_list)\n",
        "plt.plot(test_acc_list)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe5e32f60b8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nu0fJyFBhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}